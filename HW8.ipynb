{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f66c14ff",
   "metadata": {},
   "source": [
    "1.ChatBot History: https://chatgpt.com/share/673fd68e-3080-8000-b063-28777db3d321 \n",
    "(a)I. The type of problem a Classification Decision Tree addresses:\n",
    "A Classification Decision Tree is used to solve classification problems, where the goal is to assign labels to data points based on their features. It works by recursively splitting the dataset into branches based on feature values, leading to a decision at each node. At the leaf nodes, final classification labels are assigned. These trees are particularly helpful when the decision-making process can be represented as a sequence of conditions or \"if-then\" rules.\n",
    "   II. Examples of real-world applications where this might be particularly useful:\n",
    "   1)Medical Diagnosis: Classifying patients as having a particular disease or not based on symptoms, test results, and demographic data.\n",
    "   2)Customer Segmentation: Grouping customers into categories (e.g., loyal, at risk, or new) based on purchase history and demographics.\n",
    "   3)Fraud Detection: Identifying fraudulent transactions in financial systems by analyzing patterns in transaction data.\n",
    "   4)Credit Scoring: Predicting if a borrower is likely to default on a loan based on their credit history and financial behavior.\n",
    "   5)Email Filtering: Classifying emails as spam or not spam based on keywords, sender information, and metadata.\n",
    "\n",
    "(b)The difference between how a Classification Decision Tree makes (classification) predictions versus how Multiple Linear Regression makes (regression) predictions:\n",
    "A Classification Decision Tree predicts categorical labels by splitting data into subsets based on decision rules, creating non-linear boundaries, and assigning the most common or probable class at the leaf nodes. In contrast, Multiple Linear Regression predicts continuous values by calculating a weighted sum of input features plus an intercept, assuming a linear relationship between variables. CDTs use hierarchical \"if-then\" logic for predictions, while MLR applies mathematical coefficients for regression outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f3bab8",
   "metadata": {},
   "source": [
    "2.ChatBot History: https://chatgpt.com/share/674006ad-1dc8-8000-ab65-93269d73531e\n",
    "1)Accuracy\n",
    "  ·Scenario: \n",
    "   (a)Weather Prediction: Predicting whether it will rain tomorrow. Here, false positives (predicting rain when it doesn’t rain) and false negatives (predicting no rain when it rains) carry similar costs for users, especially in short-term planning.\n",
    "   (b)Spam Email Detection in Small Businesses: If a balanced dataset of spam vs. non-spam emails is available, accuracy gives a good overview of overall performance since misclassifications (either FP or FN) are equally inconvenient but not critical.\n",
    "  ·Rationale: Accuracy provides a straightforward measure of a model's performance when misclassification costs are equal and class imbalances do not skew the evaluation.\n",
    "\n",
    "2)Sensitivity\n",
    "  ·Scenario: \n",
    "   (a)Medical Diagnostics (e.g., Cancer Screening): Detecting cancer in its early stages is critical. Missing a true positive (false negative) could lead to delayed treatment and severe outcomes. It’s better to flag potential cases (even if some are false positives) for further testing.\n",
    "   (b)Fraud Detection: In financial systems, it’s crucial to identify as many fraudulent transactions as possible. Missing a fraud case could result in significant financial losses, even if it means investigating some legitimate transactions.\n",
    "  ·Rationale: Sensitivity ensures that the system minimizes false negatives, which is vital when the cost of failing to identify a positive case outweighs the inconvenience of investigating false positives.\n",
    "   \n",
    "3)Specificity\n",
    "  ·Scenario:\n",
    "   (a)Spam Email Filtering for Critical Communication Systems: In an email system used for emergency or critical communication (e.g., healthcare or disaster response), falsely marking legitimate emails as spam (FP) could cause significant issues. High specificity ensures most legitimate emails pass through while tolerating some spam.\n",
    "   (b)Legal Background Checks: When screening individuals for sensitive roles, it’s crucial to avoid falsely labeling someone as high-risk (FP), as it could lead to unfair rejection. High specificity minimizes such errors, even if it means missing some true positives.\n",
    "  ·Rationale: Specificity is essential when false positives carry significant consequences, such as unnecessary actions, reputational damage, or resource wastage.\n",
    "  \n",
    "4)Precision\n",
    "  ·Scenario:\n",
    "   (a)Hiring Algorithms (Identifying Qualified Candidates): When an automated system selects candidates for a job interview, precision ensures that most shortlisted candidates are genuinely qualified. A high false-positive rate could waste time and resources on unqualified candidates.\n",
    "   (b)Online Recommendation Systems: For recommending high-priority content (e.g., critical notifications or premium product offers), precision ensures that the recommendations are relevant and well-targeted to user preferences. Irrelevant recommendations (FP) may frustrate users.\n",
    "  ·Rationale: Precision focuses on the correctness of positive predictions, making it crucial in applications where acting on false positives incurs high costs or risks inefficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ff1021",
   "metadata": {},
   "source": [
    "3.ChatBot History: https://chatgpt.com/share/674008a2-c640-8000-a4c0-e4c0823f1b93\n",
    "After cleaning the Amazon books dataset by removing certain columns, dropping rows with missing values, and adjusting data types, we performed an initial analysis. We checked the dataset’s structure and summary statistics for the Pub year and NumPages columns. Histograms showed the distribution of publication years and page counts. We also examined the Hard_or_Paper column to see how many books were hardcover or paperback. Lastly, we looked at the correlation between Pub year and NumPages. This analysis provides a basic understanding of the dataset's key features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b1b570c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 319 entries, 0 to 324\n",
      "Data columns (total 10 columns):\n",
      " #   Column         Non-Null Count  Dtype   \n",
      "---  ------         --------------  -----   \n",
      " 0   Title          319 non-null    object  \n",
      " 1   Author         319 non-null    object  \n",
      " 2   List Price     319 non-null    float64 \n",
      " 3   Amazon Price   319 non-null    float64 \n",
      " 4   Hard_or_Paper  319 non-null    category\n",
      " 5   NumPages       319 non-null    int64   \n",
      " 6   Publisher      319 non-null    object  \n",
      " 7   Pub year       319 non-null    int64   \n",
      " 8   ISBN-10        319 non-null    object  \n",
      " 9   Thick          319 non-null    float64 \n",
      "dtypes: category(1), float64(3), int64(2), object(4)\n",
      "memory usage: 25.4+ KB\n",
      "None\n",
      "       List Price  Amazon Price    NumPages     Pub year       Thick\n",
      "count  319.000000    319.000000  319.000000   319.000000  319.000000\n",
      "mean    18.362978     12.941034  334.272727  2002.175549    0.903448\n",
      "std     13.976755     12.436673  161.601510    10.646133    0.365261\n",
      "min      1.500000      0.770000   24.000000  1936.000000    0.100000\n",
      "25%     13.890000      8.600000  208.000000  1998.000000    0.600000\n",
      "50%     15.000000     10.200000  320.000000  2005.000000    0.900000\n",
      "75%     19.360000     12.560000  416.000000  2010.000000    1.100000\n",
      "max    139.950000    139.950000  896.000000  2011.000000    2.100000\n",
      "Hard_or_Paper\n",
      "P    233\n",
      "H     86\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_52/2098781759.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ab_reduced_noNaN['Pub year'] = ab_reduced_noNaN['Pub year'].astype(int)\n",
      "/tmp/ipykernel_52/2098781759.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ab_reduced_noNaN['NumPages'] = ab_reduced_noNaN['NumPages'].astype(int)\n",
      "/tmp/ipykernel_52/2098781759.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ab_reduced_noNaN['Hard_or_Paper'] = ab_reduced_noNaN['Hard_or_Paper'].astype('category')\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://raw.githubusercontent.com/pointOfive/STA130_F23/main/Data/amazonbooks.csv\"\n",
    "ab = pd.read_csv(url, encoding=\"ISO-8859-1\")\n",
    "\n",
    "# Step 1: Drop the specified columns (Weight_oz, Width, Height)\n",
    "columns_to_remove = ['Weight_oz', 'Width', 'Height']\n",
    "ab_reduced = ab.drop(columns=columns_to_remove)\n",
    "\n",
    "# Step 2: Drop rows with any NaN entries\n",
    "ab_reduced_noNaN = ab_reduced.dropna()\n",
    "\n",
    "# Step 3: Set data types for specific columns\n",
    "ab_reduced_noNaN['Pub year'] = ab_reduced_noNaN['Pub year'].astype(int)\n",
    "ab_reduced_noNaN['NumPages'] = ab_reduced_noNaN['NumPages'].astype(int)\n",
    "ab_reduced_noNaN['Hard_or_Paper'] = ab_reduced_noNaN['Hard_or_Paper'].astype('category')\n",
    "\n",
    "# Display a summary of the processed dataset\n",
    "print(ab_reduced_noNaN.info())\n",
    "print(ab_reduced_noNaN.describe())\n",
    "print(ab_reduced_noNaN['Hard_or_Paper'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8745003b",
   "metadata": {},
   "source": [
    "4.ChatBot History: https://chatgpt.com/share/67400b77-5ebc-8000-8611-2031e614d67d\n",
    "y = pd.get_dummies(ab_reduced_noNaN[\"Hard_or_Paper\"])['H']: this step is encoding the Hard_or_Paper column into a binary format, where 1 represents a hardcover book and 0 represents a paperback book.\n",
    "X = ab_reduced_noNaN[['List Price']]: This step is essentially selecting the feature (List Price) that will be used by the model to make predictions about whether a book is a hardcover or paperback.\n",
    "\n",
    "To train a DecisionTreeClassifier using the List Price variable to predict whether a book is hardcover or paperback, we first prepare the target variable (y) by encoding the Hard_or_Paper column into a binary format (1 for hardcover, 0 for paperback), and select the List Price as the feature (X). We initialize the classifier with a maximum depth of 2 and train it using the fit() method. Finally, the trained tree is visualized using plot_tree(), which shows the decision boundaries based on List Price for classifying the books.\n",
    "\n",
    "The decision tree visualizes how the model predicts the book's type (hardcover or paperback) based on price thresholds determined by the List Price. For example, if the tree splits at $20, then books with a List Price above $20 may be predicted as hardcover, while those below may be predicted as paperback."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb1fb83",
   "metadata": {},
   "source": [
    "5.Updated ChatBot History: https://chatgpt.com/share/67400b77-5ebc-8000-8611-2031e614d67d\n",
    "X = ab_reduced_noNaN[['NumPages', 'Thick', 'List Price']]: This line selects the predictor variables (features) that the decision tree will use to make its predictions.\n",
    "max_depth = 4: This means that the decision tree will have at most 4 levels of decisions. In other words, the tree can make up to 4 splits based on the features (i.e., it can follow 4 decision nodes before reaching the final prediction).\n",
    "\n",
    "How predictions are made for the clf2 model:\n",
    "1)The decision tree classifier makes predictions by sequentially splitting the data based on feature values.\n",
    "2)At each decision node, the tree checks the value of a feature, divides the data, and continues down the tree.\n",
    "3)Eventually, the tree reaches a leaf node where it makes a final prediction based on the majority class in that node.\n",
    "4)The final output of the model is the predicted class (e.g., hardcover or paperback) for the new data point based on how it follows the splits in the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc09f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Split the data into training and test sets (already done in the previous step)\n",
    "ab_reduced_noNaN_train, ab_reduced_noNaN_test = train_test_split(\n",
    "    ab_reduced_noNaN, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare the target variable (y) and features (X)\n",
    "y = pd.get_dummies(ab_reduced_noNaN_train[\"Hard_or_Paper\"])['H']  # Target: hardcover or paperback\n",
    "X = ab_reduced_noNaN_train[['NumPages', 'Thick', 'List Price']]  # Features: NumPages, Thick, List Price\n",
    "\n",
    "# Initialize the DecisionTreeClassifier with max_depth=4\n",
    "clf2 = DecisionTreeClassifier(max_depth=4, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "clf2.fit(X, y)\n",
    "\n",
    "# Visualize the decision tree\n",
    "plot_tree(clf2, feature_names=['NumPages', 'Thick', 'List Price'], class_names=['Paperback', 'Hardcover'], filled=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1137be8",
   "metadata": {},
   "source": [
    "6.Updated ChatBot History: https://chatgpt.com/share/67400b77-5ebc-8000-8611-2031e614d67d\n",
    "1)For clf model:\n",
    "Sensitivity: 32/(32+8) = 0.8\n",
    "Specificity: 50/(50+10) = 0.833\n",
    "Accuracy: (32+50)/(32+50+10+8) = 0.82\n",
    "2)For clf2 model:\n",
    "Sensitivity: 33/(33+7) = 0.825\n",
    "Specificity: 52/(52+8) = 0.867\n",
    "Accuracy: (33+52)/(33+52+8+7) = 0.85"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb949bc",
   "metadata": {},
   "source": [
    "7.\n",
    "The differences between the two confusion matrices can be attributed to the input features used for training the decision tree model. In the first confusion matrix, the model is trained using only the List Price variable, which likely leads to a simpler and potentially less accurate decision boundary. In contrast, the second confusion matrix uses a broader set of features (NumPages, Thick, and List Price), which provides more information for the model to base its predictions on, leading to potentially more accurate predictions.\n",
    "\n",
    "The confusion matrices for clf and clf2 are better because they use optimized sets of features (NumPages, Thick, and List Price), allowing the models to make more informed predictions. The inclusion of additional features and tuning of hyperparameters, like setting max_depth=4 in clf2, helps the models capture more complex patterns in the data, resulting in better classification performance (higher sensitivity, specificity, and accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61253450",
   "metadata": {},
   "source": [
    "8.\n",
    "To visualize the feature importances for clf2, you can use the clf2.feature_importances_ attribute to examine the relative importance of each predictor variable. By plotting these importances in a bar chart, with the features labeled using clf2.feature_names_in_, you can easily identify which features are most influential in the decision tree model. The predictor variable with the highest importance score is the most crucial for making predictions. This visualization helps in understanding which features the model relies on most for its classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e3d767",
   "metadata": {},
   "source": [
    "9.\n",
    "In linear regression, coefficients represent the strength and direction of the relationship between each predictor variable and the outcome, with larger coefficients indicating a stronger influence on the dependent variable. These coefficients are interpretable as the change in the outcome variable for a one-unit change in the predictor, assuming all other variables remain constant. In decision trees, feature importances quantify the relative contribution of each feature to reducing impurity (e.g., Gini impurity or entropy) in the tree, reflecting how much a feature helps in making more accurate splits in the data, but they don't provide a direct, linear interpretation of the effect of each feature on the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76521b80",
   "metadata": {},
   "source": [
    "10.Yes, I have."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
